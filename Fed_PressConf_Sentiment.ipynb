{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06263f87",
   "metadata": {},
   "source": [
    "# Fed Press Conference Audio → Sentiment → Intraday SP500 Alignment \n",
    "\n",
    "**Notebook purpose:** End‑to‑end pipeline to (1) download a Fed press-conference video, (2) segment audio into fixed windows, (3) transcribe with Whisper, (4) score sentiment with FinBERT, (5) align with SP500 intraday prices with a 15s embargo, and (6) perform a simple Pearson correlation analysis.\n",
    "\n",
    "> **Inspiration:** This notebook is inspired by Chapter 9 of Generative AI for Trading and Asset Management by Hamlet Medina and Ernest P. Chan. It reproduces the high-level system described there for educational purposes, with additional pragmatic glue code and commentary to make it runnable in practice.\n",
    "\n",
    "> **Disclaimer:** This notebook is provided solely for educational purposes and does not constitute financial advice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba550a",
   "metadata": {},
   "source": [
    "## 0) Environment, Dependencies and Project Configuration\n",
    "- Python 3.9+ is recommended.\n",
    "- ffmpeg must be installed and available on your PATH (for audio I/O).\n",
    "- A GPU is helpful for Whisper but not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1296558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649fc5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "YOUTUBE_URL = \"https://www.youtube.com/watch?v=u0V3gnOjOi0\"\n",
    "AUDIO_FILE = \"audio.mp4\"        \n",
    "AUDIO_FILE_CLIP = \"clip.mp4\"        \n",
    "OUT_DIR, CHUNK_DIR = Path(\"out\"), Path(\"out/chunks\")\n",
    "AUDIO_FILE_CONVERTED = OUT_DIR/\"audio.wav\"\n",
    "CHUNK, EMBARGO = 60, 15  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46782f90",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Download press‑conference audio from YouTube\n",
    "\n",
    "We grab the **best available audio-only** stream with `pytubefix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50977080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytubefix import YouTube\n",
    "\n",
    "yt = YouTube(YOUTUBE_URL)\n",
    "\n",
    "audio_stream = yt.streams.filter(only_audio=True).order_by(\"abr\").desc().first()\n",
    "audio_stream.download(filename=AUDIO_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a71fe",
   "metadata": {},
   "source": [
    "## 2) Segment audio into fixed windows (“audio bars”)\n",
    "\n",
    "(1) Clip the audio to include only the speech segments, (2) Convert the audio to mono at 16 kHz for ASR, (3) Split the audio into 1-minute chunks with a 15-second embargo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import ffmpeg\n",
    "\n",
    "start = \"00:56:44\"\n",
    "end = \"01:02:23\"\n",
    "fmt = \"%H:%M:%S\"\n",
    "start_dt = dt.datetime.strptime(start, fmt)\n",
    "new_start_dt = max(start_dt - dt.timedelta(seconds=EMBARGO), dt.datetime.strptime(\"00:00:00\", fmt))\n",
    "new_start = new_start_dt.strftime(fmt)\n",
    "\n",
    "subprocess.run([\n",
    "    \"C:/ffmpeg/bin/ffmpeg.exe\",\n",
    "    \"-ss\", new_start,\n",
    "    \"-to\", end,\n",
    "    \"-i\", AUDIO_FILE,\n",
    "    \"-c\", \"copy\",\n",
    "    AUDIO_FILE_CLIP\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de94190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffmpeg.input(AUDIO_FILE_CLIP).output(str(AUDIO_FILE_CONVERTED), ac=1, ar=16000).overwrite_output().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur = float(subprocess.check_output(\n",
    "    [\"ffprobe\",\"-v\",\"error\",\"-show_entries\",\"format=duration\",\n",
    "     \"-of\",\"default=noprint_wrappers=1:nokey=1\",\"out/audio.wav\"]\n",
    ").decode().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks, start = [], 0\n",
    "while start + CHUNK <= dur+1e-6:\n",
    "    out_wav = CHUNK_DIR/f\"chunk_{int(start):04d}.wav\"\n",
    "    (ffmpeg.input(str(AUDIO_FILE_CLIP), ss=start, t=CHUNK)\n",
    "     .output(str(out_wav)).overwrite_output().run())\n",
    "    chunks.append((out_wav, start))\n",
    "    start += CHUNK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e439e4",
   "metadata": {},
   "source": [
    "## 3) Transcribe segments with Whisper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024426b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "rows = []\n",
    "for wav,s0 in chunks:\n",
    "    txt = whisper_model.transcribe(str(wav), fp16=False, verbose=False).get(\"text\",\"\").strip()\n",
    "    rows.append({\"chunk_start_s\":s0,\"transcript\":txt})\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176da564",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37389ec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['transcript'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c7bf5c",
   "metadata": {},
   "source": [
    "## 4) Sentiment analysis with FinBERT (prosusai/finbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "clf = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(\"cpu\").eval()\n",
    "\n",
    "labels = clf.config.id2label  # e.g., {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "print(labels)\n",
    "pos_idx = next(i for i, v in labels.items() if v.lower().startswith(\"pos\"))\n",
    "neg_idx = next(i for i, v in labels.items() if v.lower().startswith(\"neg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5295538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    enc = tok(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        p = torch.softmax(clf(**enc).logits, dim=-1).numpy()[0]\n",
    "    return p[pos_idx] - p[neg_idx]\n",
    "df[\"sentiment\"] = df[\"transcript\"].apply(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a651a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b922a1ca",
   "metadata": {},
   "source": [
    "## 5) Align with SP500 intraday prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c1d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_data = {\n",
    "    \"time\": [\n",
    "        \"2025-07-30 15:31:00\",\n",
    "        \"2025-07-30 15:32:00\",\n",
    "        \"2025-07-30 15:33:00\",\n",
    "        \"2025-07-30 15:34:00\",\n",
    "        \"2025-07-30 15:35:00\",\n",
    "        \"2025-07-30 15:36:00\"\n",
    "    ],\n",
    "    \"close\": [\n",
    "        6392.41,\n",
    "        6392.11,\n",
    "        6391.11,\n",
    "        6393.62,\n",
    "        6396.32,\n",
    "        6389.22\n",
    "    ]\n",
    "}\n",
    "sp = pd.DataFrame(sp_data)\n",
    "sp[\"time\"] = pd.to_datetime(sp[\"time\"])\n",
    "sp[\"log_return\"] = np.log(sp[\"close\"] / sp[\"close\"].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d3ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.concat([sp.dropna().reset_index(drop=True), df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa51987",
   "metadata": {},
   "source": [
    "## 6) Pearson correlation analysis between SP500 prices and sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b7746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd7e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.regplot(x=\"close\", y=\"sentiment\", data=temp, marker=\"o\", line_kws={\"color\": \"red\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a386b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(temp[\"log_return\"], temp[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ef1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
